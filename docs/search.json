[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "rd3c3_guide",
    "section": "",
    "text": "Preface\nrd3c3 is A wrapper function library designed to fit model-based invariance analysis and alignment methods analysis for international large-scale assessments, using graded response models with a probit link.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-install",
    "href": "index.html#how-to-install",
    "title": "rd3c3_guide",
    "section": "How to install",
    "text": "How to install\n\n# -----------------------------------------------\n# install rd3c3\n# -----------------------------------------------\n\ndevtools::install_github(\ndevtools::install_github(\n  'dacarras/rd3c3',\n  force = TRUE\n)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-cite-the-current-document",
    "href": "index.html#how-to-cite-the-current-document",
    "title": "rd3c3_guide",
    "section": "How to cite the current document",
    "text": "How to cite the current document\nCarrasco, Diego; Sandoval-Hernández, Andrés; Eryilmaz, Nurullah (2024). Guidelines for measurement invariance and aligment methods using library(rd3c3). figshare. Software. https://doi.org/10.6084/m9.figshare.28028564",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgement",
    "href": "index.html#acknowledgement",
    "title": "rd3c3_guide",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis work was supported by the IEA Research and Development Fund.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 References\nDimitrov, D. M. (2010). Testing for Factorial Invariance in the Context of Construct Validation. Measurement and Evaluation in Counseling and Development, 43, 121–149. https://doi.org/10.1177/0748175610373459\nKline, R. B. (2023). Principles and Practice of Structural Equation Modeling (5th ed.). Guilford Press.\nMuthén, B., & Asparouhov, T. (2014). IRT studies of many groups: the alignment method. Supplemental Material. Frontiers in Psychology, 5, 23–31. https://doi.org/10.3389/fpsyg.2014.00978\nSvetina, D., Rutkowski, L., & Rutkowski, D. (2020). Multiple-Group Invariance with Categorical Outcomes Using Updated Guidelines: An Illustration Using Mplus and the lavaan/semTools Packages. Structural Equation Modeling: A Multidisciplinary Journal, 27(1), 111–130. https://doi.org/10.1080/10705511.2019.1602776\nThissen, D. (2024). A Review of Some of the History of Factorial Invariance and Differential Item Functioning. Multivariate Behavioral Research, 0(0), 1–25. https://doi.org/10.1080/00273171.2024.2396148\nTse, W. W. Y., Lai, M. H. C., & Zhang, Y. (2024). Does strict invariance matter? Valid group mean comparisons with ordered-categorical items. Behavior Research Methods, 56(4), 3117–3139. https://doi.org/10.3758/s13428-023-02247-6\nWang, J., & Wang, X. (2020). Confirmatory Factor Analysis. In Structural Equation Modeling: Applications Using Mplus (pp. 33–117). John Wiley & Sons, Inc. https://doi.org/10.4324/9781315832746-25\nWu, H., & Estabrook, R. (2016). Identification of Confirmatory Factor Analysis Models of Different Levels of Invariance for Ordered Categorical Outcomes. Psychometrika, 81(4), 1014–1045. https://doi.org/10.1007/s11336-016-9506-0\nWu, M., Tam, H. P., & Jen, T.-H. (2016). Educational Measurement for Applied Researchers. Springer Singapore. https://doi.org/10.1007/978-981-10-3302-5",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "section_1_response_model.html",
    "href": "section_1_response_model.html",
    "title": "2  Response model",
    "section": "",
    "text": "2.1 Graded Response Model\nThe graded response model can be found in the literature also under the name of confimatory factor analysis for ordinal indicators, the two parameter normal ogive form of the graded response model (Wang & Wang, 2020), and as a graded response model with probit link (Bovaird & Koziol, 2012). The graded response model is a response model propose for ordinal indicators, proposed by Samejima (1968). Historically, it appears before the partial credit model (Masters, 1982), which is the most popular response model to generate scores across several large scale assessment studies (Carrasco, Irribarra & González, 2022). This model includes different model variants. These variants include the homogenous and the heterogeneous case (Samejima, 2016). The homogenous case, is a model where factor loadings (or item slopes) are fixed to be common across items; while the heteorogenous case leaves the item slope parameters to vary freely. Moreover, this model can be specified with different link functions, the logit function and the probit function (Bovaird & Koziol, 2012).\nWe will review the formal presentation of these two variants, so is easier to make a bridge between polytomous item response theory models, and confirmatory factor models. Formally, these two models can be expressed with the following equations:\n\\[\nPr(y_{ip} &gt; k) = \\frac{exp[a_{i}(\\theta_{p}-b_{ik})]}{exp[1 + a_{i}(\\theta_{p}-b_{ik})]}\n\\tag{2.1}\\]\nThe GRM model with a logit link expressed the probability of responses \\(y\\) to item \\(i\\) from person \\(p\\) as a ratio. In the exponentiated numerator we have the propensity to choose the ordered categories in a direction ( \\(\\theta_{p}\\) ), minus the boundary category parameter \\(b\\) for the \\(i\\)th item category \\(k\\) or higher, multiplied by the slope of the items \\(a_{i}\\). The parameter \\(a_{i}\\) is often interpreted as a discrimination parameter, because the higher is its value, the higher is the separation between low and high attribute persons in their expected response probability. In the denominator of the previous formula, we repeat the previous term, the exponentiation of the propensity minus the boundary category parameter times the slope, plus a unity. The present formula can be expressed in a more concise manner, by calling the logit link function.\n\\[\nlogit[Pr(y_{ip} \\leq k)] = a_{i}(\\theta_{p}-b_{ik})\n\\tag{2.2}\\]\nGraded response models (GRM) with logit link are very similar to partial credit model (PCM), under the homogeneous case. In the homogenous case, the \\(a_{i}\\) can be constrained to one, and then only the person locations ( \\(\\theta_{p}\\) ) and item locations ( \\(b_{ik}\\) ) are included in the model. The main difference between these two models is their logit link function. While the PCM includes the adjacent logit link; the GRM relies on the cumulative category link (Mellenbergh, 1994). Thus, for items with three ordered response categories, the item locations are the natural logarithms of the odds of answering 1 vs 2, and 2 vs 3 for the adjacent logit link; while for the cumulative link function consists of natural logarithms contrasting the odds of answering 1 vs 2, 3; and 1, 2 vs 3 (Carrasco et al., 2022).\nAn alternative formulation for the present model and the focus of the present guideline is teh GRM with the probit link. Following Bovaird & Koziol (2012), we express this model with the next equation:\n\\[\nPr(y_{ip} &gt; k) = \\phi(-\\tau_{ik} + \\lambda_{i}\\theta_{p})\n\\tag{2.3}\\]\nSimilary to the previous equation, we can express the previous formula in a more concise manner by using the probit link in the equation:\n\\[\nprobit[Pr(y_{ip} &gt; k)] = \\tau_{ik} - \\lambda_{i}\\theta_{p}\n\\tag{2.4}\\]\nThis second formulation is more akin to the confirmatory factor analysis tradition, where is common to include item intercepts (i.e., item location parameters), item slopes (i.e., factor loadings) and a term for the theoretical factor.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Response model</span>"
    ]
  },
  {
    "objectID": "section_1_response_model.html#invariance-model-specifications-with-the-grm-model",
    "href": "section_1_response_model.html#invariance-model-specifications-with-the-grm-model",
    "title": "2  Response model",
    "section": "2.2 Invariance model specifications with the GRM model",
    "text": "2.2 Invariance model specifications with the GRM model\nA response model can be considered invariant if all reponse model parameters are can be held equal across groups, besides the group latent means. This general idea applies to polytomous item response models such as confirmatory factor analysis, graded response models (e.g., Wu & Estabrook, 2016; Tse et al., 2024) and to mixture variants of response models (e.g., Masyn, 2017; Torres Irribarra & Carrasco, 2021); that is response models with latent factors that are discrete instead of normally distributed (Torres Irribarra, 2021). This is the most demanding for equivalence of responses models between groups, usually described as strict invariance. A more relaxed version of invariance model specification is scalar invariance, in which all response model parameters are held equal across groups, beside latent means and item uniqueness or scale factors (e.g., Grimm et al. 2016; Tse et al., 2024). Moreover, model specification with more parameters allow to vary freely are not able to provide latent mean comparisons. These includes models with common thresholds but free factor loadings, and purely descriptive models were all response models parameters are allowed to vary freely.\nA common practice in measurement invariance with CFA for continous indicators is to start with the model with less constrains (e.g., Dimitrov, 2010), and continue further till the most constrained model (i.e., strict invariance). In essence, this is a model building sequence (Kline, 2023). In this sequence, difference model specifications are included. The first, is the configural model specification where only the model sructure is common, yet all response model parameters vary freely between groups. Then is followed by the metric model specification where only factor loadings are held equal, ye, there are no parameters in the multigroup model to compare latent means between groups (e.g. Wu & Estabrook, 2016). Hence, this model spefication is similar to a latent centered means. In a third stage, the scalar model specification is included. This model helds as common parameters factor loadings and model indicator intercepts and include latent mean constraints. This model allows for latent mean comparison among groups. Finally, the most constrained model specification, the strict model held as common parameters all response model parameters, with the exemption of latent means. Thus, allowing to compare groups on latent means, while assuming residual error of the response model are common among groups.\nModel specification sequence for assessing invariance on CFA with ordinal indicators, is different from CFA wih continous indicators. Wu & Estabrook (2016) asserts that invariance within the CFA for ordinal indicators, common thresholds are needed before common factor loadings can be introduced in the model building sequence (Wu & Estabrook, 2016; Svetina, et al. 2020; Tse, et al, 2024). In practice, common factor loadings between group cannot be tested alone (Wu & Estabrook, 2016, p1023). Complementary, Tse et al. (2024) recommends to assess if strict invariance holds among groups, before relying on total scores (e.g., observed means) for group comparisons. Then, if strict invariance fails, then proceed to search for partially invariant solutions such as, partially strict invariance, and scalar invariance if latent means can be used instead of observed mean scores. Following Tse et al. (2024) one can alter the model sequence for a model trimming sequence instead (Kline, 2023). That is, instead of starting with the model with the most freely estimated parameters, one can start with the model with the most held equal parameters among groups. As such, the model sequence for GRM would be: strict, scalar, configural (with common thresholds), and a base model (with freely estimated response model parameters).\nIn the following figure, we summarize the parameters of the response models that can be held equal between groups in each of the model specification for CFA with continous and for CFA with ordinal indicators.\n\n\n\n\n\nFigure 1: response model parameters being held equal in each model specification.\n\n\n\n\nThe present table is a rough summary of the different response model parameters that are held equal among groups, to specify each model solution. For example, in Wu & Estabrook (2016) the configural model specification for the CFA with ordinal indicators consists of a model where thresholds are held equal among groups. This is the baseline model from which model comparisons can be made in contrast to scalar and strict solutions of the GRM model. However, the configural model has more constraints than solely common thresholds, this includes factor means constrained to zero (i.e., centered) and factor variances fix to 1 on all groups (see Svetina et al., 2020). Additionally, Tse et al. (2024) discussess alternative model specification for the configural solution, using the theta parametrization in which factor loadings are held common between groups, and threshols are held common for marker indicators. In the present guidelines we will review these model specifications in more detail in section 4, following Svetina et al. (2020) and Wu & Estabrook (2016).\nIt should be clear that model specifications propose for CFA with continous indicators are not equivalent for other response models. The weak invariance (e.g., Dimitrov, 2010) or metric invariance model specification (Wu & Estabrook, 2016), where common factor loadings are held equal across groups, do not reach a model specification that holds the same interpretation of traditional CFA, for CFA with ordinal indicators (Wu & Estabrook, 2016; Svetina, et al. 2020; Tse, et al, 2024). A similar observation can be done for the assumed interpretation of the metric model specification of latent class models (e.g., Hooghe & Oser, 2015; Hooghe et al. 2016), which is an special case of a non-invariant solution (Masyn, 2017), and doesn’t hold the same interpretation of the random term across groups, the configuration of the laten classess (Torres Irribarra, et al., 2021). If invariance holds, the purpose is to assert that group differences are on the random term of the response model (i.e., factors, latent means, latent classes); in contrast, if the model specification doesn’t provide group differences estimates on the same scale, then subtantive conclusions are not tenable, because these do not have a common meaning between groups. In summary, the interpretation one can hold over response models fitted between groups with varying equality constrains among groups are not equivalent between response models.\nIn the following section (section 2) we will describe what are partially invariant solutions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Response model</span>"
    ]
  },
  {
    "objectID": "section_1_response_model.html#references",
    "href": "section_1_response_model.html#references",
    "title": "2  Response model",
    "section": "2.3 References",
    "text": "2.3 References\nCarrasco, D., Irribarra, D. T., & González, J. (2022). Continuation Ratio Model for Polytomous Items Under Complex Sampling Design. In Quantitative Psychology (pp. 95–110). https://doi.org/10.1007/978-3-031-04572-1_8\nDimitrov, D. M. (2010). Testing for Factorial Invariance in the Context of Construct Validation. Measurement and Evaluation in Counseling and Development, 43, 121–149. https://doi.org/10.1177/0748175610373459\nBovaird, J. A., & Koziol, N. A. (2012). Measurement Models for Ordered-Categorical Indicators. In R. H. Hoyle (Ed.), Handbook of Structural Equation Modeling (pp. 495–511). Guilford Press.\nGrimm, K. J., & Liu, Y. (2016). Residual Structures in Growth Models With Ordinal Outcomes. Structural Equation Modeling, 23(3), 466–475. https://doi.org/10.1080/10705511.2015.1103192\nHooghe, M., & Oser, J. (2015). The rise of engaged citizenship: The evolution of citizenship norms among adolescents in 21 countries between 1999 and 2009. International Journal of Comparative Sociology, 56(1), 29–52. https://doi.org/10.1177/0020715215578488.\nHooghe, M., Oser, J., & Marien, S. (2016). A comparative analysis of ‘good citizenship’: A latent class analysis of adolescents’ citizenship norms in 38 countries. International Political Science Review, 37(1), 115–129. https://doi.org/10.1177/0192512114541562.\nKline, R. B. (2023). Principles and Practice of Structural Equation Modeling (5th ed.). Guilford Press.\nMasters, G. N. (1982). A rasch model for partial credit scoring. Psychometrika, 47(2), 149–174. https://doi.org/10.1007/BF02296272\nMasyn, K. E. (2017). Measurement Invariance and Differential Item Functioning in Latent Class Analysis With Stepwise Multiple Indicator Multiple Cause Modeling. Structural Equation Modeling: A Multidisciplinary Journal, 24(2), 180–197. https://doi.org/10.1080/10705511.2016.1254049\nSamejima, F. (1968). Estimation of latent Ability using a response pattern of graded scores. ETS Research Bulletin Series, 1968(1), i–169. https://doi.org/10.1002/j.2333-8504.1968.tb00153.x\nSamejima, F. (2016). Graded Response Models. In W. J. van der Linden (Ed.), Handbook of Item Response Theory. Volume One. Models (pp. 95–107). CRC Press. https://doi.org/10.1201/9781315374512-16\nTorres Irribarra, D. (2021). A Pragmatic Perspective of Measurement. Springer International Publishing. https://doi.org/10.1007/978-3-030-74025-2\nTorres Irribarra, D., & Carrasco, D. (2021). Profiles of Good Citizenship. In E. Treviño, D. Carrasco, E. Claes, & K. J. Kennedy (Eds.), Good Citizenship for the Next Generation. A Global Perspective Using IEA ICCS 2016 Data (pp. 33–50). Springer International Publishing. https://doi.org/10.1007/978-3-030-75746-5_3\nTse, W. W. Y., Lai, M. H. C., & Zhang, Y. (2024). Does strict invariance matter? Valid group mean comparisons with ordered-categorical items. Behavior Research Methods, 56(4), 3117–3139. https://doi.org/10.3758/s13428-023-02247-6\nWang, J., & Wang, X. (2020). Confirmatory Factor Analysis. In Structural Equation Modeling: Applications Using Mplus (pp. 33–117). John Wiley & Sons, Inc. https://doi.org/10.4324/9781315832746-25\nWu, H., & Estabrook, R. (2016). Identification of Confirmatory Factor Analysis Models of Different Levels of Invariance for Ordered Categorical Outcomes. Psychometrika, 81(4), 1014–1045. https://doi.org/10.1007/s11336-016-9506-0",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Response model</span>"
    ]
  },
  {
    "objectID": "section_2_partial.html",
    "href": "section_2_partial.html",
    "title": "3  Partial invariance",
    "section": "",
    "text": "3.1 References\nAsparouhov, T., & Muthén, B. (2014). Multiple-group factor analysis alignment. Structural Equation Modeling: A Multidisciplinary Journal, 21(4), 495–508. https://doi.org/10.1080/10705511.2014.919210\nAsparouhov, T., & Muthén, B. (2022). Multiple group alignment for exploratory and structural equation models. Structural Equation Modeling: A Multidisciplinary Journal, 1–23. https://doi.org/10.1080/10705511.2022.2127100\nFischer, J., Praetorius, A. K., & Klieme, E. (2019). The impact of linguistic similarity on cross-cultural comparability of students’ perceptions of teaching quality. Educational Assessment, Evaluation and Accountability, 31, 201–220.\nGilbert, J. B. (2024). Modeling item-level heterogeneous treatment effects: A tutorial with the glmer function from the lme4 package in R. Behavior Research Methods, 56(5), 5055–5067. https://doi.org/10.3758/s13428-023-02245-8\nLeitgöb, H., Seddig, D., Asparouhov, T., Behr, D., Davidov, E., De Roover, K., Jak, S., Meitinger, K., Menold, N., Muthén, B., Rudnev, M., Schmidt, P., & van de Schoot, R. (2023). Measurement invariance in the social sciences: Historical development, methodological challenges, state of the art, and future perspectives. Social Science Research, 110(October 2022). https://doi.org/10.1016/j.ssresearch.2022.102805\nMeredith, W. (1993). Measurement invariance, factor analysis, and factorial invariance. Psychometrika, 58(4), 525–543. https://doi.org/10.1007/BF02294825\nMillsap, R. E. (2011). Statistical Approaches to Measurement Invariance. New York, NY: Routledge.\nMuthén, B., & Asparouhov, T. (2014). IRT studies of many groups: the alignment method. Supplemental Material. Frontiers in Psychology, 5, 23–31. https://doi.org/10.3389/fpsyg.2014.00978\nPokropek, A., Lüdtke, O., & Robitzsch, A. (2020). An extension of the invariance alignment method for scale linking. Psychological Test and Assessment Modeling, 62(2), 305–334.\nRobitzsch, A., & Lüdtke, O. (2023). Why full, partial, or approximate measurement invariance are not a prerequisite for meaningful and valid group comparisons. Structural Equation Modeling, 30(2), 190–204. https://doi.org/10.1080/10705511.2023.2191292\nSvetina, D., Rutkowski, L., & Rutkowski, D. (2020). Multiple-group invariance with categorical outcomes using updated guidelines: An illustration using Mplus and the lavaan/semTools packages. Structural Equation Modeling: A Multidisciplinary Journal, 27(1), 111–130. https://doi.org/10.1080/10705511.2019.1602776\nVan de Schoot, R., Lugtig, P., & Hox, J. (2012). A checklist for testing measurement invariance. European Journal of Developmental Psychology, 9(4), 486–492. https://doi.org/10.1080/17405629.2012.686740\nVan De Vijver, F.J.R., Matsumoto, D. (2011) Introduction to the methodological issues associated with cross-cultural research. In: Matsumoto, D., van de Vijver, F.J.R. (Eds.), Cross-Cultural Research Methods in Psychology, 1st ed..,. Cambridge University Press, pp. 1–14.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Partial invariance</span>"
    ]
  },
  {
    "objectID": "section_3_rd3c3.html",
    "href": "section_3_rd3c3.html",
    "title": "4  Library overview",
    "section": "",
    "text": "4.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Library overview</span>"
    ]
  },
  {
    "objectID": "section_3_rd3c3.html#summary",
    "href": "section_3_rd3c3.html#summary",
    "title": "4  Library overview",
    "section": "",
    "text": "library(rd3c3) is a collection of wrapper functions\nthese wrapper functions are included within template to produce item analysis reports\nthe item analysis reports provide different statistical and psychometric results of interest to make judgments regarding the quality of scale scores",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Library overview</span>"
    ]
  },
  {
    "objectID": "section_3_rd3c3.html#rd3c3-as-a-collection-of-wrappers",
    "href": "section_3_rd3c3.html#rd3c3-as-a-collection-of-wrappers",
    "title": "4  Library overview",
    "section": "4.2 rd3c3 as a collection of wrappers",
    "text": "4.2 rd3c3 as a collection of wrappers\nThe library(rd3c3) is a collection of wrapper functions (Stanton, 2017) that helps to streamline the task of genereting code to fit different response model specifications. Wrapper functions are a way to call more complex functions and code, into a simpler user interface. The main aim of the library is to ease the coding time needed when assess measurement invariance of polytomous batteries of items, from background questionnaires of large scale assessment studies. The different model specifications follow Wu & Estabrook (2014), Svetina, Rutkowski & Rutkowski (2020) and Tse, Lai & Chang (2024) recommendations to fit different multigroup models of the graded response model with probit link, to fit strict, scalar, common threshold and base (i.e., descriptive) multigroup models. Moreover, the library also contain functions to apply aligment methods onto the same response model among groups. It relies on MplusAutomation (Hallquist & Wiley, 2018), to fit response models using Mplus (Muthén & Muthén, 2017), so all fitted models can take into account sample design features of large scale assessment studies such as stratification variables, clustering variables, and survey weights (e.g.., Stapleton, 2013).\nThe handler name of the library is rd3c3, and stems from the research development call number 3 (rd3), for cross cultural comparison (c3).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Library overview</span>"
    ]
  },
  {
    "objectID": "section_3_rd3c3.html#code-applicable-to-a-set-of-items",
    "href": "section_3_rd3c3.html#code-applicable-to-a-set-of-items",
    "title": "4  Library overview",
    "section": "4.3 Code applicable to a set of items",
    "text": "4.3 Code applicable to a set of items\nMost of the wrapper functions included in library(rd3c3) are not intended to be use onto sole objects, such as vectors or data frames. These are design to be fitted onto a set of elements, define in a table. Once the table, which we call generally scale_info, is filled in and called into the R session, the wrapper functions can resolve which items are subject to an analysis, within a define data object a particular data frame. We illustrate the general logic of the wrapper functions with the following diagram.\n\n\n\n\n\nFigure 2: wrapper function logic\n\n\n\n\nFor the wrapper functions to work, one needs to provide:\n\na scale_info table\nto specify within the table the data_file name with responses of interest\nchoose a particular scale, via a scale_id\n\nAs such, different wrapper functions have the same arguments in the following form:\n\nrd3c3::name_of_the_function(\n  data = data_responses, \n  scale_num = scale_id, \n  scale_info = scales_data\n)\n\nThis is the case for different functions within rd3c3. For example, to name a few:\n\nrd3c3::get_descriptives()\n\nis a wrapper function to produce nominal descriptives of items\n\nrd3c3::missing_summary()\n\nit generates a summary of missing data across items, distinguising complete, partial and missing response patterns by observations\n\nrd3c3::fit_grm2()\n\nis a wrapper function that fits a graded response model with probit link onto a set of items declared in the scale_info table\n\nrd3c3::ctt_table()\n\nit produces classical test theory statistics such as biserial correlatiosn and alpha if deleted for set of items declared in the scale_info table\n\n\nFor example, a the rd3c3::get_descriptives() function generates item descriptives including means, standard deviations, and histograms; and instead of being applicable to a matrix of responses is applied onto a whole data object. Thanks to the additional arguments included in the wrapper function, the code is able to select the items that are indexed with a scale_id number within the scale_info table. Thus, just by specifing the desired scale_id the user can get the results for these collection of items contained in a particular data file.\n\n\nCode\n#------------------------------------------------------------------------------\n# descriptives\n#------------------------------------------------------------------------------\n\n# -----------------------------------------------\n# scale id\n# -----------------------------------------------\n\nrd3c3::silent(library(dplyr))\n\n# -----------------------------------------------\n# scale id\n# -----------------------------------------------\n\nscale_id &lt;- 1\n\n# -----------------------------------------------\n# scales info\n# -----------------------------------------------\n\nscales_data &lt;- readxl::read_xlsx(\n               'guideline_scale_info_example.xlsx', \n               sheet = 'scales_data'\n               )\n\n# -----------------------------------------------\n# data file\n# -----------------------------------------------\n\ndata_file &lt;- scales_data %&gt;%\ndplyr::filter(scale_num == scale_id) %&gt;%\ndplyr::select(data_file) %&gt;%\nunique() %&gt;%\ndplyr::pull() \n\n# -----------------------------------------------\n# response matrix\n# -----------------------------------------------\n\ndata_responses &lt;- readRDS(data_file)\n\n# -----------------------------------------------\n# descriptives\n# -----------------------------------------------\n\nrd3c3::get_descriptives(\n  data = data_responses, \n  scale_num = scale_id, \n  scale_info = scales_data) %&gt;%\ndplyr::select(var, missing, complete, n, mean, sd, skew, kurt, hist) %&gt;%\nknitr::kable(., \n  digits = 2,\n  caption = 'Table 1: descriptives example')\n\n\n\nTable 1: descriptives example\n\n\nvar\nmissing\ncomplete\nn\nmean\nsd\nskew\nkurt\nhist\n\n\n\n\nBSBG13A\n0.04\n0.96\n7480\n2.13\n0.84\n0.61\n2.97\n▃▁▇▁▁▂▁▁\n\n\nBSBG13B\n0.04\n0.96\n7480\n1.82\n0.77\n0.78\n3.34\n▆▁▇▁▁▂▁▁\n\n\nBSBG13C\n0.05\n0.95\n7480\n1.84\n0.83\n0.83\n3.16\n▇▁▇▁▁▂▁▁\n\n\nBSBG13E\n0.04\n0.96\n7480\n1.88\n0.85\n0.81\n3.12\n▇▁▇▁▁▂▁▁\n\n\n\n\n\nThe generated results are the descriptives of items BSBG13A, BSBG13B, BSBG13C, BSBG13E from the “Sense of School Belonging” scale, present in TIMSS 2019, for Chile and England.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Library overview</span>"
    ]
  },
  {
    "objectID": "section_3_rd3c3.html#template-based-workflow",
    "href": "section_3_rd3c3.html#template-based-workflow",
    "title": "4  Library overview",
    "section": "4.4 Template based workflow",
    "text": "4.4 Template based workflow\nThe library(rd3c3) is intended to produce item analys reports of polytomous scales, in the spirit of dynamic reports (Xie, 2017). These are reproducible statistical analysis, that fills-in a define template. For any scale_id, a collection of items, one can generate an item analys report that include different sets of results.\nThis is in stark contrast to a manually coded workflow, where the user needs to code ever function to set of specific items within a data frame, many times to build a single item analysis report to every scale. In comparison, a template based workflow already contains an opinionated set of analysis (Parker, 2017) selected with a purpose. In this case, to make judgments of quality of scale scores in terms of unidimensionality, reliability, comparability and inference limitations. The following diagram depicts the contrast between these two manners to reach the set of intended item analysis reports.\n\n\n\n\n\nFigure 3: wrapper function logic within a dynamic report\n\n\n\n\nIn the manually coded workflow the user needs to code each set of expected results for each set of items. While in the second workflow, the template based workflow, the user just needs to choose the respective scale_id and can get the intended item analys reports.\nThe logic of a template is that allows a user to get a series of results regarding the responses to a collection of items that are intended to measure a known attribute (i.e., a construct). This templates includes a selection of the different results a researcher can use to make judgements regarding the quality of a total score from polytomous items scale.\nA template for assesing the quality of scale scores should follow different design data analysis principles such as reproducibility and exhaustivity (McGowan et al., 2023). This template should be reproducible in the sense that other user, with the same data file, library, and needed software (i.e., Mplus). Thus, such a template should be able to get the same results present in a generated report. Ideally a template for item analysis reports should follows the design principle of exhaustivity, in the sense of including different results that helps to make a judgment regarding the quality of the total score one can produce with the responses to a set of indicators. Using library(rd3c3) the user can include results from a graded response models, under different model specifications (i.e., strict, scalar, configural, base), alignment methods for graded response models, and partial credit model for item fit.\nIn essence, to build a dynamic item analysis report, the user needs to define the scale_info, define the data_responses of interest, and by using library(rd3c3) within a define template where the user includes all the statistical analysis relevant for its purpose. As a whole, the user can generate dynamic results reports per scale. The following diagram summarize the minimal elements to produce these dynamic reports.\n\n\n\n\n\nFigure 4: dynamic reports",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Library overview</span>"
    ]
  },
  {
    "objectID": "section_3_rd3c3.html#main-pieces-of-evidence-to-judge-the-quality-of-scales-scores",
    "href": "section_3_rd3c3.html#main-pieces-of-evidence-to-judge-the-quality-of-scales-scores",
    "title": "4  Library overview",
    "section": "4.5 Main pieces of evidence to judge the quality of scales scores",
    "text": "4.5 Main pieces of evidence to judge the quality of scales scores\nThe main pieces of evidence that librar(rd3c3) can produce for item analysis reports are:\n\nUnidimensionality\nReliability\nComparability\nInference limitations\n\nUnidimensionality is judge based on parallel analysis for ordinal indicators (Lubbe, 2019).\nReliability of scale scores is judge by inspecting the distribution of errors of the latent factor of the GRM model, its summary via person separation reliability (Verhavert et al., 2018) and via the Cronbach’s alpha index (Cronbach, 1951).\nComparability among participating countries is assess via the results of the model based measurement invariance results (Wu & Estabrook, 2016; Svetina, Rutkowski & Rutkowski, 2020; Tse, Lai & Chang, 2024), and the complementary results of the alignment analysis results (Muthén & Asparouhov, 2014).\nInference limitations can be made based on an holistic judgment of previous results, regarding to which locations of the latent continuum the scale score is more informative, via inspection of the person item map, and the distribution of standard errors of the theta realizations of the response model. For example if the distribution of item location is concentraded in one of the tails a researcher can identify possible ceiling or floor effects of the scale (e.g., Carrasco, Rutkowski, and Rutkowski, 2023). Moreover, the item analysis reports provides results of measurement invariance and alignment analysis providing information regarding the tenability of assuming a common response model being held equal among the compared groups. Thus, the researcher can spot scales scores where the comparison among groups may not be guarantee and further research is needed to isolate points of support for comparisons, by iterating the decisions on item and group selections and exclusions.\nIn the following section (section 4), we illustrate the application of the library(rd3c3) to produce invariance analysis, and build an item analysis report with the described characteristics.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Library overview</span>"
    ]
  },
  {
    "objectID": "section_3_rd3c3.html#references",
    "href": "section_3_rd3c3.html#references",
    "title": "4  Library overview",
    "section": "4.6 References",
    "text": "4.6 References\nCarrasco, D., Rutkowski, D., & Rutkowski, L. (2023). The advantages of regional large-scale assessments: Evidence from the ERCE learning survey. International Journal of Educational Development, 102(May), 102867. https://doi.org/10.1016/j.ijedudev.2023.102867\nCronbach, L. J. (1951). Coefficient alpha and the internal structure of tests. Psychometrika, 16(3), 297–334. https://doi.org/10.1007/BF02310555\nHallquist, M. N., & Wiley, J. F. (2018). MplusAutomation: An R Package for Facilitating Large-Scale Latent Variable Analyses in Mplus. Structural Equation Modeling: A Multidisciplinary Journal, 25(4), 621–638. https://doi.org/10.1080/10705511.2017.1402334\nLubbe, D. (2019). Parallel analysis with categorical variables: Impact of category probability proportions on dimensionality assessment accuracy. Psychological Methods, 24(3), 339–351. https://doi.org/10.1037/met0000171\nMcGowan, L. D. A., Peng, R. D., & Hicks, S. C. (2023). Design principles for data analysis. Journal of Computational and Graphical Statistics, 32(2), 754-761.\nMuthén, L. K., & Muthén, B. O. (2017). Mplus User’s Guide (8th ed.). Muthén & Muthén.\nParker, H. (2017). Opinionated analysis development (pp. 1–13). https://doi.org/10.7287/peerj.preprints.3210\nStanton, J. M. (2017). Reasoning with Data. An Introduction to Traditional and Bayesian Statistics Using R. Guilford Press.\nStapleton, L. M. (2013). Incorporating Sampling Weights into Single- and Multilevel Analyses. In L. Rutkowski, M. von Davier, & D. Rutkowski (Eds.), Handbook of International Large scale Assessment: background, technical issues, and methods of data analysis (pp. 363–388). Chapman and Hall/CRC.\nSvetina, D., Rutkowski, L., & Rutkowski, D. (2020). Multiple-Group Invariance with Categorical Outcomes Using Updated Guidelines: An Illustration Using Mplus and the lavaan/semTools Packages. Structural Equation Modeling: A Multidisciplinary Journal, 27(1), 111–130. https://doi.org/10.1080/10705511.2019.1602776\nTse, W. W. Y., Lai, M. H. C., & Zhang, Y. (2024). Does strict invariance matter? Valid group mean comparisons with ordered-categorical items. Behavior Research Methods, 56(4), 3117–3139. https://doi.org/10.3758/s13428-023-02247-6\nVerhavert, S., De Maeyer, S., Donche, V., & Coertjens, L. (2018). Scale separation reliability: What does it mean in the context of comparative judgment?. Applied Psychological Measurement, 42(6), 428-445.\nWu, H., & Estabrook, R. (2016). Identification of Confirmatory Factor Analysis Models of Different Levels of Invariance for Ordered Categorical Outcomes. Psychometrika, 81(4), 1014–1045. https://doi.org/10.1007/s11336-016-9506-0\nXie, Y. (2017). Dynamic Documents with R and knitr. CRC Press.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Library overview</span>"
    ]
  },
  {
    "objectID": "section_4_example.html",
    "href": "section_4_example.html",
    "title": "5  Applied Examples",
    "section": "",
    "text": "5.1 Summary\n# -----------------------------------------------\n# install rd3c3\n# -----------------------------------------------\n\ndevtools::install_github(\n  'dacarras/rd3c3',\n  force = TRUE\n)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applied Examples</span>"
    ]
  },
  {
    "objectID": "section_4_example.html#summary",
    "href": "section_4_example.html#summary",
    "title": "5  Applied Examples",
    "section": "",
    "text": "We use data from TIMSS 2019, from the student background questionnaire.\n\nIn particular we are using student responses to the items of the scale “Sense of School Belonging”, from Chile and England.\nThe data file includes 4115 students from Chile, and 3365 students from England, from 8th grade\nWe are using a taylored made data file, where we include specific clustering and survey design variables:\n\nid_i = unique id number for students\nid_j = unique id number for schools\nid_s = unique id number for stratification factors (i.e., JKZONES)\nid_k = unique id number for country samples\nws = scale survey weights to a constant of 1000\ndata_ex1.rds\n\n\nTo install the library, a user can use the following code, to download the development version of the library:\n\n\n\nWe include three applied examples:\n\nInvariance Analysis\n\nModel based invariance analysis\nAlignment method analsyis\n\nItem report analysis\n\nThe item report analysis includes a larger set of analysis, besides Model based invariance analysis and Alignment method analsyis, such as item descriptives, missing data analysis, parallel analysis and reliability analysis among others.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applied Examples</span>"
    ]
  },
  {
    "objectID": "section_4_example.html#invariance-analysis",
    "href": "section_4_example.html#invariance-analysis",
    "title": "5  Applied Examples",
    "section": "5.2 Invariance analysis",
    "text": "5.2 Invariance analysis\n\n5.2.1 Model based invariance analysis\nModel based invariance analysis includes five model specifications:\n\npooled is a graded response model with probit link, including survey design variables (i.e., stratification factors, primary sampling unit and student survey weights) (e.g., Stapleton, 2013). Is a single response model, fitted onto pooled sample of cases, were sampling weights have been scale to a common constant so including countries contributes equally to estimates (Gonzalez, 2012).\nstrict is a multigroup graded response model, where all response model parameters are held equal between compared groups (Tse et al., 2024), including the scale factors.\nscalar is a multigroup graded response model, where all response model parameters are held equal among groups, with the exemption of scale factors. It follows model specifications described in Svetina et al. (2020, proposition 7).\nconfigural is a multigroup graded response model, where thresholds are held common among the compared countries (i.e., threshold invariance). Is the baseline model for a model building sequence for assesing model based measurement invariance (Wu & Estabrook, 2016). It follows model specifications described in Svetina et al. (2020, proposition 4).\nbase is a a multigroup graded response model, where all response model parameters are freely estimated among the compare groups.\n\nSimulations studies from Rutkowski & Svetina (2017) with the graded response model with probit link and a larger amount of compare groups (e.g., 10, 20) suggest that RMSEA &lt; .055 serves as a rule of thumb to select well fitting response models with invariant parameters among the compare groups.\nTo fit the following models we use as inputs:\n\ndata_responses data_ex1.rds\nscale_info = guideline_scale_info_example.xlsx\nscale_id = 1\nand is using the functions:\n\nrd3c3::fit_grm2 for the pooled model\nrd3c3::fit_grm2_m01_strict for the strict model\nrd3c3::fit_grm2_m02_scalar for the scalar model\nrd3c3::fit_grm2_m03_configural for the configural model\nrd3c3::fit_grm2_m04_base for the base model (i.e., descriptive model)\n\n\nIn the following section we include code (folded) to produce the invariance model fit indexes table (see Table 1).\n\n\nCode\n#------------------------------------------------------------------------------\n# define objects\n#------------------------------------------------------------------------------\n\n# -----------------------------------------------\n# scale id\n# -----------------------------------------------\n\nrd3c3::silent(library(dplyr))\n\n# -----------------------------------------------\n# scale id\n# -----------------------------------------------\n\nscale_id &lt;- 1\n\n# -----------------------------------------------\n# scales info\n# -----------------------------------------------\n\nscales_data &lt;- readxl::read_xlsx(\n               'guideline_scale_info_example.xlsx', \n               sheet = 'scales_data'\n               )\n\n# -----------------------------------------------\n# data file\n# -----------------------------------------------\n\ndata_file &lt;- scales_data %&gt;%\ndplyr::filter(scale_num == scale_id) %&gt;%\ndplyr::select(data_file) %&gt;%\nunique() %&gt;%\ndplyr::pull() \n\n# -----------------------------------------------\n# response matrix\n# -----------------------------------------------\n\ndata_responses &lt;- readRDS(data_file) %&gt;%\nmutate(grp = paste0(COUNTRY)) %&gt;%\nmutate(grp = as.numeric(as.factor(COUNTRY))) %&gt;%\nmutate(grp_name = paste0(COUNTRY))\n\n#------------------------------------------------------------------------------\n# response models\n#------------------------------------------------------------------------------\n\n# -----------------------------------------------\n# most centered\n# -----------------------------------------------\n\ngrp_centered &lt;- 'CHL'\n\n# -----------------------------------------------\n# pooled\n# -----------------------------------------------\n\ninv_0 &lt;- rd3c3::silent(\n             rd3c3::fit_grm2(\n           data = data_responses,\n           scale_num  = scale_id,\n             scale_info = scales_data\n              )\n             )\n\n# -----------------------------------------------\n# strict\n# -----------------------------------------------\n\ninv_1 &lt;- rd3c3::silent(\n           rd3c3::fit_grm2_m01_strict(\n           data = data_responses, \n           scale_num = scale_id, \n           scale_info = scales_data,\n           grp_var = 'id_k',\n           grp_txt = 'grp_name',\n           grp_ref = grp_centered\n           )\n           )\n\n# -----------------------------------------------\n# scalar\n# -----------------------------------------------\n\ninv_2 &lt;- rd3c3::silent(\n           rd3c3::fit_grm2_m02_scalar(\n           data = data_responses, \n           scale_num = scale_id, \n           scale_info = scales_data,\n           grp_var = 'id_k',\n           grp_txt = 'grp_name',\n           grp_ref = grp_centered\n           )\n           )\n\n# -----------------------------------------------\n# configural\n# -----------------------------------------------\n\ninv_3 &lt;- rd3c3::silent(\n           rd3c3::fit_grm2_m03_config(\n           data = data_responses, \n           scale_num = scale_id, \n           scale_info = scales_data,\n           grp_var = 'id_k',\n           grp_txt = 'grp_name',\n           grp_ref = grp_centered\n           )\n           )\n\n# -----------------------------------------------\n# base\n# -----------------------------------------------\n\ninv_4 &lt;- rd3c3::silent(\n           rd3c3::fit_grm2_m04_base(\n           data = data_responses, \n           scale_num = scale_id, \n           scale_info = scales_data,\n           grp_var = 'id_k',\n           grp_txt = 'grp_name',\n           grp_ref = grp_centered\n           )\n           )\n\n# -----------------------------------------------\n# retrieve fit indexes per model\n# -----------------------------------------------\n\nfit_0 &lt;- rd3c3::get_inv_fit(inv_0, model_name = 'pooled')\nfit_1 &lt;- rd3c3::get_inv_fit(inv_1, model_name = 'strict')\nfit_2 &lt;- rd3c3::get_inv_fit(inv_2, model_name = 'scalar')\nfit_3 &lt;- rd3c3::get_inv_fit(inv_3, model_name = 'config')\nfit_4 &lt;- rd3c3::get_inv_fit(inv_4, model_name = 'base')\n\n# -----------------------------------------------\n# general table\n# -----------------------------------------------\n\nfit_table &lt;- dplyr::bind_rows(\n             dplyr::select(fit_0, model, RMSEA, CFI, TLI, SRMR, x2, df, p_val),\n             dplyr::select(fit_1, model, RMSEA, CFI, TLI, SRMR, x2, df, p_val),\n             dplyr::select(fit_2, model, RMSEA, CFI, TLI, SRMR, x2, df, p_val),\n             dplyr::select(fit_3, model, RMSEA, CFI, TLI, SRMR, x2, df, p_val),\n             dplyr::select(fit_4, model, RMSEA, CFI, TLI, SRMR, x2, df, p_val)\n             )\n\n\n# -----------------------------------------------\n# model fit\n# -----------------------------------------------\n\nfit_table %&gt;%\nknitr::kable(., \n  digits = c(0,3,2,2,2,2,0,2),\n    caption = 'Table 1: invariance model fit indexes between compared groups'\n  )\n\n\n\nTable 1: invariance model fit indexes between compared groups\n\n\nmodel\nRMSEA\nCFI\nTLI\nSRMR\nx2\ndf\np_val\n\n\n\n\npooled\n0.034\n1.00\n1.00\n0.01\n18.61\n2\n0\n\n\nstrict\n0.042\n0.99\n1.00\n0.02\n112.97\n15\n0\n\n\nscalar\n0.027\n1.00\n1.00\n0.01\n40.66\n11\n0\n\n\nconfig\n0.032\n1.00\n1.00\n0.01\n37.18\n8\n0\n\n\nbase\n0.044\n1.00\n0.99\n0.01\n31.78\n4\n0\n\n\n\n\n\n\nNote: pooled = is the response model fitted onto the pooled sample. strict = is a multigroup response model with common thresholds, common loadings, and a common scale. This response model suffice mean score comparisons (Tse et al., 2024). scalar = is a multigroup esponse model with common thresholds, common loadings, and free scales for each item. This models supports latent mean comparisons (Tse et al., 2024). config = is a multigroup response model with common thresholds. base = is a multigroup descriptive model where all response model parameter are free to vary. Metric model specification is not identified under graded response models (Wu & Estabrook, 2016), thus metric solution is not included. A RMSEA of .055 or less has been found to be good threshold for fit for graded response models with many groups of 10 or 20 compared groups (see Rutkowski & Svetina, 2017).\n\n\n\n5.2.2 Alignment\nThe aligment method is optimizing for the least discrepance response model parameters among the compared groups. Is fitting a graded response model with probit link, and using the most optimal group as a reference. We are using the statement ALIGNMENT = FIXED(*); within Mplus for these purposes. We rely on the Measurement invariance explorer (https://github.com/MaksimRudnev/MIE.package) to retrieve aligment results.\nThe following code (folded) is using as inputs:\n\ndata_responses data_ex1.rds\nscale_info = guideline_scale_info_example.xlsx\nscale_id = 1\nand is using the function rd3c3::fit_grm2_align_wlsmv() to run an alignment method analysis\n\n\n\nCode\n#------------------------------------------------------------------------------\n# define objects\n#------------------------------------------------------------------------------\n\n# -----------------------------------------------\n# scale id\n# -----------------------------------------------\n\nrd3c3::silent(library(dplyr))\n\n# -----------------------------------------------\n# scale id\n# -----------------------------------------------\n\nscale_id &lt;- 1\n\n# -----------------------------------------------\n# scales info\n# -----------------------------------------------\n\nscales_data &lt;- readxl::read_xlsx(\n               'guideline_scale_info_example.xlsx', \n               sheet = 'scales_data'\n               )\n\n# -----------------------------------------------\n# data file\n# -----------------------------------------------\n\ndata_file &lt;- scales_data %&gt;%\ndplyr::filter(scale_num == scale_id) %&gt;%\ndplyr::select(data_file) %&gt;%\nunique() %&gt;%\ndplyr::pull() \n\n# -----------------------------------------------\n# response matrix\n# -----------------------------------------------\n\ndata_responses &lt;- readRDS(data_file) %&gt;%\nmutate(grp = paste0(COUNTRY)) %&gt;%\nmutate(grp = as.numeric(as.factor(COUNTRY))) %&gt;%\nmutate(grp_name = paste0(COUNTRY))\n\n#------------------------------------------------------------------------------\n# alignment\n#------------------------------------------------------------------------------\n\n# -----------------------------------------------\n# aligned\n# -----------------------------------------------\n\n\nfitted_align &lt;- rd3c3::silent(\nrd3c3::fit_grm2_align_wlsmv(\ndata = data_responses, \nscale_num = scale_id, \nscale_info = scales_data)\n)\n\n# -----------------------------------------------\n# retrieve output\n# -----------------------------------------------\n\nscale_file &lt;- scales_data %&gt;%\ndplyr::filter(scale_num == scale_id) %&gt;%\ndplyr::select(mplus_file) %&gt;%\nunique() %&gt;%\ndplyr::pull() \n\n\nalignment_out &lt;- MIE::extractAlignment(paste0(scale_file,'_align.out'), silent = TRUE)\n\n# -----------------------------------------------\n# display\n# -----------------------------------------------\n\nalignment_table &lt;- alignment_out$summary %&gt;%\n                   tibble::rownames_to_column(\"terms\") %&gt;%\n                   tibble::as_tibble() %&gt;%\n                   rename(\n                   term        = 1,\n                   a_par       = 2,\n                   R2          = 3,\n                   n_inv       = 4,\n                   n_dis       = 5,\n                   inv_grp     = 6,\n                   dis_grp     = 7\n                   ) %&gt;%\n                   mutate(type = case_when(\n                   stringr::str_detect(term, 'Threshold') ~ 'tau',\n                   stringr::str_detect(term, 'Loadings')  ~ 'lambda'\n                   )) %&gt;%              \n                   mutate(term = stringr::str_replace(term, '\\\\$', '_')) %&gt;%\n                   mutate(term = stringr::str_replace(term, 'Threshold', '')) %&gt;%\n                   mutate(term = stringr::str_replace(term, 'Loadings', '')) %&gt;%\n                   mutate(term = stringr::str_replace(term, 'ETA by ', '')) %&gt;%\n                   dplyr::select(\n                   type, term, a_par, R2, n_inv, n_dis, inv_grp, dis_grp)\n\n# -----------------------------------------------\n# display\n# -----------------------------------------------\n\nalignment_table %&gt;%\nknitr::kable(., \n  digits = 2,\n    caption = 'Table 2: alignment comparisons'\n)\n\n\n\nTable 2: alignment comparisons\n\n\ntype\nterm\na_par\nR2\nn_inv\nn_dis\ninv_grp\ndis_grp\n\n\n\n\ntau\nI01_1\nNA\nNA\n0\n2\n\n18 11\n\n\ntau\nI01_2\nNA\nNA\n0\n2\n\n18 11\n\n\ntau\nI01_3\n0.73\n1.00\n2\n0\n11 18\n\n\n\ntau\nI02_1\n-1.89\n0.94\n2\n0\n11 18\n\n\n\ntau\nI02_2\n-1.09\n0.00\n2\n0\n11 18\n\n\n\ntau\nI02_3\nNA\nNA\n0\n2\n\n18 11\n\n\ntau\nI03_1\n-1.73\n0.83\n2\n0\n11 18\n\n\n\ntau\nI03_2\n-1.00\n0.91\n2\n0\n11 18\n\n\n\ntau\nI03_3\n0.20\n0.85\n2\n0\n11 18\n\n\n\ntau\nI05_1\n-1.64\n0.88\n2\n0\n11 18\n\n\n\ntau\nI05_2\n-0.97\n0.01\n2\n0\n11 18\n\n\n\ntau\nI05_3\n0.23\n0.93\n2\n0\n11 18\n\n\n\nlambda\nI01\n0.72\n0.00\n2\n0\n11 18\n\n\n\nlambda\nI02\n0.77\n0.20\n2\n0\n11 18\n\n\n\nlambda\nI03\n0.84\n0.26\n2\n0\n11 18\n\n\n\nlambda\nI05\n0.80\n0.00\n2\n0\n11 18",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applied Examples</span>"
    ]
  },
  {
    "objectID": "section_4_example.html#item-report-analysis",
    "href": "section_4_example.html#item-report-analysis",
    "title": "5  Applied Examples",
    "section": "5.3 Item report analysis",
    "text": "5.3 Item report analysis\nIn the following section we include a template example, to produce item analysis reports. This template includes:\n\nScale description\n\na presentation of the name of the collection of items (i.e., the scale name)\na presentation of items as these where presented to the participants\na table with the item text, with the public data file names, and the shortened variable names\n\nAnalysis of responses\n\ndescriptives\nmissing data descriptives\n\nResponse model\n\ndimensionality analysis via parallel analysis for ordinal indicators (Lubbe, 2019)\nresponse model parameters for a graded response model\nreliability analysis\nitem person maps\n\nItem analysis\n\nitem test correlation\nitem fit based on partial credit model\n\nComparability\n\nmodel based measurement invariance\nalignment analysis of GRM among groups\n\n\nThe following figure depicts an overview of the generated report.\n\n\n\n\n\nFigure 5: overview of a dynamic item report\n\n\n\n\nTo produce this examplary report the user needs as inputs:\n\ndata_responses data_ex1.rds\nscale_info = guideline_scale_info_example.xlsx\nscale_id = 1\nthe template guideline_item_report_example.rmd\n\nand the word template report_template.docx\n\n\nThe end product of this procedure can be inspected in the following file guideline_item_report_example.docx",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applied Examples</span>"
    ]
  },
  {
    "objectID": "section_4_example.html#additional-examples",
    "href": "section_4_example.html#additional-examples",
    "title": "5  Applied Examples",
    "section": "5.4 Additional examples",
    "text": "5.4 Additional examples\nThe current examples are just toy examples, to illustrate the basic capabilities of the library. We include two other with more realistic characteristics. One example with three countries, were is possible to see that three countries do not obtained strict invariance (example with “Sense of School Belonging” for three countries). And a third example of a template based workflow, where we procede with a full fledge item analysis report including all participating countries (example with “Bullying Scale” for all participating countries).\n\nExample with “Sense of School Belonging” for three countries\n\ndata: data_example.rds\ncode template: template_example_2.rmd\n\nword template: report_template.docx\n\nresulting report: template_example_2.docx\n\nExample with “Bullying Scale” for all participating countries\n\ndatasurvey_1_g8.rds\ncode template: template_example_3.rmd\n\nword template: report_template.docx\n\nresulting report template_example_3.docx.\n\n\nAll example files can be downladed from the following link.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applied Examples</span>"
    ]
  },
  {
    "objectID": "section_4_example.html#references",
    "href": "section_4_example.html#references",
    "title": "5  Applied Examples",
    "section": "5.5 References",
    "text": "5.5 References\nGonzalez, E. J. (2012). Rescaling sampling weights and selecting mini-samples from large-scale assessment databases. IERI Monograph Series Issues and Methodologies in Large-Scale Assessments, 5, 115–134.\nTse, W. W. Y., Lai, M. H. C., & Zhang, Y. (2024). Does strict invariance matter? Valid group mean comparisons with ordered-categorical items. Behavior Research Methods, 56(4), 3117–3139. https://doi.org/10.3758/s13428-023-02247-6\nStapleton, L. M. (2013). Incorporating Sampling Weights into Single- and Multilevel Analyses. In L. Rutkowski, M. von Davier, & D. Rutkowski (Eds.), Handbook of International Large scale Assessment: background, technical issues, and methods of data analysis (pp. 363–388). Chapman and Hall/CRC.\nSvetina, D., Rutkowski, L., & Rutkowski, D. (2020). Multiple-Group Invariance with Categorical Outcomes Using Updated Guidelines: An Illustration Using Mplus and the lavaan/semTools Packages. Structural Equation Modeling: A Multidisciplinary Journal, 27(1), 111–130. https://doi.org/10.1080/10705511.2019.1602776\nWu, M., Tam, H. P., & Jen, T.-H. (2016). Educational Measurement for Applied Researchers. Springer Singapore. https://doi.org/10.1007/978-981-10-3302-5\nRutkowski, L., & Svetina, D. (2017). Measurement Invariance in International Surveys: Categorical Indicators and Fit Measure Performance. Applied Measurement in Education, 30(1). https://doi.org/10.1080/08957347.2016.1243540",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applied Examples</span>"
    ]
  },
  {
    "objectID": "section_5_intended_use.html",
    "href": "section_5_intended_use.html",
    "title": "6  Intended use",
    "section": "",
    "text": "The final section of these guidelines outlines the intended use of the alignment optimization library and emphasizes the responsibilities of the researcher in employing this tool. The primary goal of the library is to streamline and accelerate the process of generating results for multiple groups and item scales. By automating repetitive analytical tasks, the library facilitates the production of key outputs necessary for assessing the fit or misfit of measurement invariance models across various contexts.\nHowever, it is crucial to underscore that the library is not a substitute for sound judgment and expertise of the researcher and user. While it efficiently produces the primary results needed for evaluation, it does not draw definitive conclusions regarding whether models meet specific thresholds or satisfy measurement invariance criteria. This critical interpretive step remains the responsibility of the researcher and the library user, who must apply their expertise to analyze and contextualize the findings appropriately.\nThe library is specifically designed to assist users in generating foundational results for evaluating measurement invariance. It is not intended for purposes outside this scope. It should not be used to automate decision-making regarding the acceptability or applicability of response models. Users must approach the outputs with caution, ensuring that the analyses are tailored to the specific goals of their research and the nuances of their studies.\nIn conclusion, this library is a powerful tool to enhance efficiency in measurement invariance analyses, but it is not a substitute for thorough methodological understanding and critical interpretation. Researchers are encouraged to use this resource judiciously and within its intended purpose, recognizing its limitations and their own role in ensuring the validity and reliability of their conclusions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intended use</span>"
    ]
  }
]